{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e56297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importanto as bibliotecas necessárias\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import (ensemble, preprocessing, tree)\n",
    "from sklearn.metrics import(auc, confusion_matrix, roc_auc_score,roc_curve)\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ConfusionMatrix #vou chamar quando for usar. Rolou um BO no início. Pois, precisei comendar todas as bibliotecas do yellowbrick até realmente usar\n",
    "from yellowbrick.classifier.rocauc import roc_auc\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Bibliotecas adicionadas durante o coding\n",
    "#import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efd180f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "# carregando o dataset\n",
    "try:\n",
    "\tdf = pd.read_csv('https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv')\n",
    "except:\n",
    "\t# Alternative: use seaborn's built-in titanic dataset\n",
    "\tdf = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "orig_df = df\n",
    "\n",
    "print(df.shape)\n",
    "# pclas: classe do passageiro (1 = primeira classe, 2 = segunda classe, 3 = terceira classe)\n",
    "# survived: 0 = No, 1 = Yes\n",
    "# name: nome do passageiro\n",
    "# sex: sexo do passageiro\n",
    "# age: idade do passageiro em anos\n",
    "# sibsp: número de irmãos/cônjuges a bordo\n",
    "# parch: número de pais/filhos a bordo\n",
    "# ticket: número do bilhete\n",
    "# fare: tarifa paga pelo passageiro\n",
    "# cabin: número da cabine\n",
    "# embarked: porto de embarque (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "# df.boat: bote salva-vida\n",
    "\n",
    "# organizando o dataset\n",
    "dir_path='data/'\n",
    "try:\n",
    "\t# Vou criar uma pasta para conter todas as tabelas e organizar melhor o projeto\n",
    "\t# exist_ok=True      Evita um erro caso o diretório já exista\n",
    "    os.makedirs('data', exist_ok=True) \n",
    "    os.makedirs('data/processed', exist_ok=True)\n",
    "    os.makedirs('data/preprocessed', exist_ok=True)\n",
    "    # Salvar o orig_df como um arquivo csv na pasta data\n",
    "    orig_df.to_csv(os.path.join(dir_path, 'orig_df.csv'), index=False) #tbm faremos o mesmo quando o df for limpo ou fracionado\n",
    "except Exception as dir_path:\n",
    "    print(f'Erro ao criar o diretório: {dir_path} e seus itens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded9e3a",
   "metadata": {},
   "source": [
    "# Estudo de dados para limpeza dos dados\n",
    "Aqui queremos trabalho sobre os NaNs ou outliers ou dados inconsistentes. Assim como queremos ermover colunas que possam gerar algum data leakege. Tbm vamos criar colunas dummy ('pd.get_dummies')caso precisemos.\n",
    "> Data leakage ocorre quando informações do conjunto de dados de teste ou validação vazam para o conjunto de treinamento durante o pré-processamento ou modelagem, ou quando informações do target vazam para as features. \n",
    "\n",
    "Nesse processo vamos utilizar duas bibliotecas do pandas \n",
    "- pandas_profiling\n",
    "- pandas_profiling.ProfileReport(df)\n",
    "Para gerar relatórios em notebooks para vermos detalhes estatísticos descritivos e dos quantis, além de histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2453398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                              Heikkinen, Miss Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "(891, 12)\n",
      "nossas colunas são: PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiago-ubunto/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Summarize dataset: 100%|██████████| 47/47 [00:01<00:00, 26.66it/s, Completed]                       \n",
      "Generate report structure: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 56.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório salvo em data/preprocessed/profile_report.html\n",
      "708 linhas com pelo menos um NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass                           Name     Sex   Age  \\\n",
       "0            1         0       3        Braund, Mr. Owen Harris    male  22.0   \n",
       "2            3         1       3          Heikkinen, Miss Laina  female  26.0   \n",
       "4            5         0       3       Allen, Mr. William Henry    male  35.0   \n",
       "5            6         0       3               Moran, Mr. James    male   NaN   \n",
       "7            8         0       3  Palsson, Master Gosta Leonard    male   2.0   \n",
       "\n",
       "   SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "2      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "4      0      0            373450   8.0500   NaN        S  \n",
       "5      0      0            330877   8.4583   NaN        Q  \n",
       "7      3      1            349909  21.0750   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age         177\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n",
      "Cabin          77.104377\n",
      "Age            19.865320\n",
      "Embarked        0.224467\n",
      "PassengerId     0.000000\n",
      "Name            0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# exibindo as primeiras linhas do dataset\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(f'nossas colunas são: {df.dtypes}')   # Estudamos os tipos de colunas que possuímos\n",
    "\n",
    "# Gerar relatório de perfil (tenta ydata_profiling, senão usa pandas_profiling)\n",
    "# Não importamos `display` diretamente aqui para evitar import errors com IPython.core.display\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pandas_profiling import ProfileReport\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Instale 'ydata-profiling' (recomendado) ou 'pandas-profiling' antes de gerar o relatório.\"\n",
    "        ) from e\n",
    "\n",
    "report = ProfileReport(df)\n",
    "# Mostrar no notebook (iframe) - to_notebook_iframe() já cuida da exibição no Jupyter\n",
    "try:\n",
    "    report.to_notebook_iframe()\n",
    "except Exception:\n",
    "    # Caso a exibição inline falhe, escrevemos o arquivo HTML e informamos o caminho\n",
    "    report.to_file(os.path.join(dir_path, \"preprocessed\", \"profile_report.html\"))\n",
    "    print(f\"Relatório salvo em {os.path.join(dir_path, 'preprocessed', 'profile_report.html')}\")\n",
    "else:\n",
    "    # também salvar em HTML\n",
    "    report.to_file(os.path.join(dir_path, \"preprocessed\", \"profile_report.html\"))\n",
    "\n",
    "# analizando o df de forma truncada as duas colunas iniciais\n",
    "df.describe().iloc[:, :2]  # Tanto `.loc` quanto `.iloc` são atributos essenciais dos DataFrames do Pandas e ambos são usados ​​para selecionar subconjuntos específicos de dados. Seu propósito é acessar e permitir a manipulação de uma parte específica do DataFrame, em vez do DataFrame inteiro.\n",
    "\n",
    "# vamos utilizar o isnull para verificar valores nulos\n",
    "df.isnull().sum()\n",
    "df.isnull().mean()\n",
    "\n",
    "# analise de algumas das linhas com dados ausentes com boleanos\n",
    "mask = df.isnull().any(axis=1)\n",
    "print(mask.sum(), \"linhas com pelo menos um NaN\")\n",
    "display(df[mask].head())           # inspeciona as linhas com NaN\n",
    "print(df.isnull().sum()[lambda s: s>0])  # colunas com NaNs e suas contagens\n",
    "print((df.isnull().mean()*100).sort_values(ascending=False).head())  # % de NaNs por coluna\n",
    "#mask.head() #linhas\n",
    "#df[mask].body.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a0e1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n",
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n",
      "a cabeça do nosso objeto names: 0                              Braund, Mr. Owen Harris\n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
      "2                                Heikkinen, Miss Laina\n",
      "Name: Name, dtype: object\n",
      "Nossas colunas são com os dummies aplicados: Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male',\n",
      "       'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# categorização para limpeza\n",
    "# defina o target antes de remover a coluna\n",
    "print(df.dtypes)\n",
    "y = df['Survived']   #Estou em dúvida se criaria outra coluna \"dead\" como target alternativo. Se pensarmos em Boleanos, seriam antônimos\n",
    "X = df.drop(columns=\"Survived\")\n",
    "print(df.dtypes)\n",
    "\n",
    "if 'Name' in df.columns:\n",
    "    name= df['Name']\n",
    "    print(f'a cabeça do nosso objeto names: {name.head(3)}')\n",
    "else:\n",
    "    print(\"A coluna do objeto Name está presente somente no dataframe original.\")\n",
    "#removendo colunas que não serão usadas ou podem gerar viés no modelo\n",
    "\n",
    "\n",
    "df=df.drop(columns=[\"Name\",\"Ticket\", \"Survived\", \"Cabin\"]) \n",
    "# logo vamos criar uma coluna \"dummy\"\n",
    "df=pd.get_dummies(df)  # ou # df=pd.get_dummies(df,drop_first=True) #para evitar a dummy trap... No caso as colunas male_sex e famale são inversos perfeitos\n",
    "df=df.drop(columns=[\"Sex_female\"])  # removendo uma das colunas dummies para evitar dummy trap\n",
    "print(f'Nossas colunas são com os dummies aplicados: {df.columns}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8895722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação de amostra #amostra_para_validação_do_modelo\n",
    "# scikit-learn para separar 30% para os testes\n",
    "# import library q usaremos abaixo\n",
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Impotação dos dados\n",
    "##como a coluna idade tem valores ausentes. Vamos imputar udade a partir dos valores numéricos apenas no conjunto de treinamento, e então"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da66e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IterativeImputer é experimental em algumas versões do sklearn:\n",
    "# importe o \"enable\" antes de importar o estimador\n",
    "from sklearn.experimental import enable_iterative_imputer  # deve vir antes DO DE BAIXO\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Colunas numéricas a imputar (use nomes como strings)\n",
    "num_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# verificar quais colunas realmente existem nos dataframes (evita KeyError)\n",
    "num_cols = [c for c in num_cols if c in X_train.columns]\n",
    "\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "X_train.loc[:, num_cols] = imputer.fit_transform(X_train[num_cols])\n",
    "X_test.loc[:, num_cols] = imputer.transform(X_test[num_cols])\n",
    "\n",
    "# preencher possíveis NaNs restantes com a mediana do conjunto de treino (aplicado às colunas numéricas)\n",
    "meds = X_train[num_cols].median()\n",
    "X_train.loc[:, num_cols] = X_train[num_cols].fillna(meds)\n",
    "X_test.loc[:, num_cols] = X_test[num_cols].fillna(meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc770223",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Dodge, Master Washington'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/tmp/ipykernel_41834/3176588993.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#normalização dos dados\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m## este processo será importante para nosso modelo ter uma melhor performance\u001b[39;00m\n\u001b[32m      3\u001b[39m cols=[\u001b[33m\"Pclass\"\u001b[39m,\u001b[33m\"Age\"\u001b[39m,\u001b[33m\"Sibsp\"\u001b[39m,\u001b[33m\"Fare\"\u001b[39m]\n\u001b[32m      4\u001b[39m sca=preprocessing.StandardScaler()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X_train=sca.fit_transform(X_train)\n\u001b[32m      6\u001b[39m X_train=pd.DataFrame(X_train, columns=X.columns)\n\u001b[32m      7\u001b[39m X_test=sca.transform(X_test)\n\u001b[32m      8\u001b[39m X_test=pd.DataFrame(X_test, columns=cols)\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m     @wraps(f)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m wrapped(self, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         data_to_wrap = f(self, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(data_to_wrap, tuple):\n\u001b[32m    318\u001b[39m             \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m             return_tuple = (\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m                 )\n\u001b[32m    904\u001b[39m \n\u001b[32m    905\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    906\u001b[39m             \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, **fit_params).transform(X)\n\u001b[32m    908\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    909\u001b[39m             \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    910\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    920\u001b[39m             Fitted scaler.\n\u001b[32m    921\u001b[39m         \"\"\"\n\u001b[32m    922\u001b[39m         \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    923\u001b[39m         self._reset()\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m self.partial_fit(X, y, sample_weight)\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/base.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1332\u001b[39m                 skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m                     prefer_skip_nested_validation \u001b[38;5;28;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m                 )\n\u001b[32m   1335\u001b[39m             ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, *args, **kwargs)\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    957\u001b[39m             Fitted scaler.\n\u001b[32m    958\u001b[39m         \"\"\"\n\u001b[32m    959\u001b[39m         xp, _, X_device = get_namespace_and_device(X)\n\u001b[32m    960\u001b[39m         first_call = \u001b[38;5;28;01mnot\u001b[39;00m hasattr(self, \u001b[33m\"n_samples_seen_\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m         X = validate_data(\n\u001b[32m    962\u001b[39m             self,\n\u001b[32m    963\u001b[39m             X,\n\u001b[32m    964\u001b[39m             accept_sparse=(\u001b[33m\"csr\"\u001b[39m, \u001b[33m\"csc\"\u001b[39m),\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2898\u001b[39m             out = y\n\u001b[32m   2899\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2900\u001b[39m             out = X, y\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2902\u001b[39m         out = check_array(X, input_name=\u001b[33m\"X\"\u001b[39m, **check_params)\n\u001b[32m   2903\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;28;01mand\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2904\u001b[39m         out = _check_y(y, **check_params)\n\u001b[32m   2905\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1019\u001b[39m                         )\n\u001b[32m   1020\u001b[39m                     array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1021\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1022\u001b[39m                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1024\u001b[39m                 raise ValueError(\n\u001b[32m   1025\u001b[39m                     \u001b[33m\"Complex data not supported\\n{}\\n\"\u001b[39m.format(array)\n\u001b[32m   1026\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m complex_warning\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/utils/_array_api.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    874\u001b[39m         \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[32m    875\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    876\u001b[39m             array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    877\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m             array = numpy.asarray(array, order=order, dtype=dtype)\n\u001b[32m    879\u001b[39m \n\u001b[32m    880\u001b[39m         \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[32m~/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2167\u001b[39m             )\n\u001b[32m   2168\u001b[39m         values = self._values\n\u001b[32m   2169\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2170\u001b[39m             \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m             arr = np.asarray(values, dtype=dtype)\n\u001b[32m   2172\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2173\u001b[39m             arr = np.array(values, dtype=dtype, copy=copy)\n\u001b[32m   2174\u001b[39m \n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Dodge, Master Washington'"
     ]
    }
   ],
   "source": [
    "#normalização dos dados\n",
    "## este processo será importante para nosso modelo ter uma melhor performance\n",
    "cols=[\"Pclass\",\"Age\",\"Sibsp\",\"Fare\"]\n",
    "sca=preprocessing.StandardScaler()\n",
    "X_train=sca.fit_transform(X_train)\n",
    "X_train=pd.DataFrame(X_train, columns=X.columns)\n",
    "X_test=sca.transform(X_test)\n",
    "X_test=pd.DataFrame(X_test, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refatoração\n",
    "## A medida que o ML for evoluindo, vamos precisar realizar ajustes e até mesmo import o original dataset novamente para refinar o modelo.\n",
    "## Faremos duas funções para o processo\n",
    "def tweak_titanic(df):\n",
    "    df=df.drop(columns=[\"Name\",\"Ticket\",\"Survived\",\"Cabin\"]).pipe(pd.get_dummies, drop_first=True)\n",
    "return df\n",
    "\n",
    "def get_train_test_X_y(df, y_col, size=0.3, std_cols=None):\n",
    "    y = df[y_col]\n",
    "    X= df.drop(columns=y_col)\n",
    "    X_train, X_test, y_train, y_test= model_selection.train_test_split( X, y, test_size=size, random_state=42)\n",
    "    cols=X.columnsnum_cols=[\"Pclass\", \"Age\", \"Sibsp\", \"Parch\", \"Fare\"]\n",
    "    fi=impute.IterativeImputer()\n",
    "    X_train.loc[:, num_cols]= fi.fit_transform(X_train[num_cols])\n",
    "    X_test.loc[:, num_cols] = fi.transform( X_test[num_cols])\n",
    "\n",
    "    if std_cols:\n",
    "        std=preprocessing.StandardScaler()\n",
    "        X_train.loc[:, std_cols] = std.fit_transform(X_train[std_cols])\n",
    "        X_test.loc[:, std_cols] = std.transform(X_test[std_cols])\n",
    "return X_train, X_test, y_train, y_test\n",
    "\n",
    "ti_df = tweaks_tweak_titanic(orig_df)\n",
    "std_cols=\"Pclass,Age,Sibsp,Fare\".split(\",\")\n",
    "X_train, X_test, y_train, y_test= get_train_test_X_y(ti_df, \"Survived\", std_cols=std_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df9afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdummy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DummyClassifier\n\u001b[32m      5\u001b[39m bm=DummyClassifier()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m bm.fit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[32m      7\u001b[39m bm.score(X_test, y_test) \u001b[38;5;66;03m#nos_dara_nossa_precisao\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metrics\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#modelo_de_base\n",
    "#uma_base_realmente_simples_que_o_modelo_usara_como_comparador\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "bm = DummyClassifier()\n",
    "bm.fit(X_train, y_train)\n",
    "bm.score(X_test, y_test) #nos_dará_nossa_precisao\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.precision_score(y_test, bm.predict(X_test))\n",
    "\n",
    "\n",
    "\n",
    "#familias\n",
    "#Neste_projeto_vamos_comparar_pontuações_AUC_e_o_desvio_padão_usando_a_validação_cruzadak_fold\n",
    "X=pd.concat([X_train, X_test])\n",
    "y=pd.concat([y_train, y_test])\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for model in [DummyClassifier, LogisticRegression, DecisionTreeClassifier, KNeighborsClassifier, GaussianNB, SVC, RandomForestClassifier, XGBClassifier]: \n",
    "    cls = model()\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=42, shuffle=True)\n",
    "    s = cross_val_score(cls, X, y, scoring=\"roc_auc\", cv=kfold)\n",
    "    print(f'{model.__name__:22} ; AUC: {s.mean():.3f} STD: {s.std():.2f}') #Por que usar o AUC em vez da Acurácia? A acurácia pode ser enganosa se os seus dados estiverem desbalanceados.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f730e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modulo\n",
    "## Usaremos um classificador de random forest\n",
    "rf=ensemble.RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliação de modelo\n",
    "rf.score(X_test, y_test)\n",
    "metrics.precision_score(y_test, rf.predict(X_test))\n",
    "\n",
    "for col, val in sorted(\n",
    "zip(X_train.columns, rf.feature_importances_,), key=lambda x: x[1], reverse=True,)[:5]):\n",
    "    print(f'{col:10}{val:10.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f07614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otimizando o modelo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Titanic (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
