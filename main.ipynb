{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7e56297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importanto as bibliotecas necessárias\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import (ensemble, preprocessing, tree)\n",
    "from sklearn.metrics import(auc, confusion_matrix, roc_auc_score,roc_curve)\n",
    "from sklearn.model_selection import (train_test_split, StratifiedKFold)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ConfusionMatrix #vou chamar quando for usar. Rolou um BO no início. Pois, precisei comendar todas as bibliotecas do yellowbrick até realmente usar\n",
    "from yellowbrick.classifier.rocauc import roc_auc\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# Bibliotecas adicionadas durante o coding\n",
    "#import pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efd180f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "# carregando o dataset\n",
    "try:\n",
    "\tdf = pd.read_csv('https://raw.githubusercontent.com/pandas-dev/pandas/master/doc/data/titanic.csv')\n",
    "except:\n",
    "\t# Alternative: use seaborn's built-in titanic dataset\n",
    "\tdf = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv')\n",
    "orig_df = df\n",
    "\n",
    "print(df.shape)\n",
    "# pclas: classe do passageiro (1 = primeira classe, 2 = segunda classe, 3 = terceira classe)\n",
    "# survived: 0 = No, 1 = Yes\n",
    "# name: nome do passageiro\n",
    "# sex: sexo do passageiro\n",
    "# age: idade do passageiro em anos\n",
    "# sibsp: número de irmãos/cônjuges a bordo\n",
    "# parch: número de pais/filhos a bordo\n",
    "# ticket: número do bilhete\n",
    "# fare: tarifa paga pelo passageiro\n",
    "# cabin: número da cabine\n",
    "# embarked: porto de embarque (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "# df.boat: bote salva-vida\n",
    "\n",
    "# organizando o dataset\n",
    "dir_path='data/'\n",
    "try:\n",
    "\t# Vou criar uma pasta para conter todas as tabelas e organizar melhor o projeto\n",
    "\t# exist_ok=True      Evita um erro caso o diretório já exista\n",
    "    os.makedirs('data', exist_ok=True) \n",
    "    os.makedirs('data/processed', exist_ok=True)\n",
    "    os.makedirs('data/preprocessed', exist_ok=True)\n",
    "    # Salvar o orig_df como um arquivo csv na pasta data\n",
    "    orig_df.to_csv(os.path.join(dir_path, 'orig_df.csv'), index=False) #tbm faremos o mesmo quando o df for limpo ou fracionado\n",
    "except Exception as dir_path:\n",
    "    print(f'Erro ao criar o diretório: {dir_path} e seus itens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded9e3a",
   "metadata": {},
   "source": [
    "# Estudo de dados para limpeza dos dados\n",
    "Aqui queremos trabalho sobre os NaNs ou outliers ou dados inconsistentes. Assim como queremos ermover colunas que possam gerar algum data leakege. Tbm vamos criar colunas dummy ('pd.get_dummies')caso precisemos.\n",
    "> Data leakage ocorre quando informações do conjunto de dados de teste ou validação vazam para o conjunto de treinamento durante o pré-processamento ou modelagem, ou quando informações do target vazam para as features. \n",
    "\n",
    "Nesse processo vamos utilizar duas bibliotecas do pandas \n",
    "- pandas_profiling\n",
    "- pandas_profiling.ProfileReport(df)\n",
    "Para gerar relatórios em notebooks para vermos detalhes estatísticos descritivos e dos quantis, além de histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2453398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                              Heikkinen, Miss Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "(891, 12)\n",
      "nossas colunas são: PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarize dataset: 100%|██████████| 47/47 [00:01<00:00, 27.73it/s, Completed]                       \n",
      "Generate report structure: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "Render HTML: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Export report to file: 100%|██████████| 1/1 [00:00<00:00, 52.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relatório salvo em data/preprocessed/profile_report.html\n",
      "708 linhas com pelo menos um NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass                           Name     Sex   Age  \\\n",
       "0            1         0       3        Braund, Mr. Owen Harris    male  22.0   \n",
       "2            3         1       3          Heikkinen, Miss Laina  female  26.0   \n",
       "4            5         0       3       Allen, Mr. William Henry    male  35.0   \n",
       "5            6         0       3               Moran, Mr. James    male   NaN   \n",
       "7            8         0       3  Palsson, Master Gosta Leonard    male   2.0   \n",
       "\n",
       "   SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "2      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "4      0      0            373450   8.0500   NaN        S  \n",
       "5      0      0            330877   8.4583   NaN        Q  \n",
       "7      3      1            349909  21.0750   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age         177\n",
      "Cabin       687\n",
      "Embarked      2\n",
      "dtype: int64\n",
      "Cabin          77.104377\n",
      "Age            19.865320\n",
      "Embarked        0.224467\n",
      "PassengerId     0.000000\n",
      "Name            0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# exibindo as primeiras linhas do dataset\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(f'nossas colunas são: {df.dtypes}')   # Estudamos os tipos de colunas que possuímos\n",
    "\n",
    "# Gerar relatório de perfil (tenta ydata_profiling, senão usa pandas_profiling)\n",
    "# Não importamos `display` diretamente aqui para evitar import errors com IPython.core.display\n",
    "try:\n",
    "    from ydata_profiling import ProfileReport\n",
    "except ImportError:\n",
    "    try:\n",
    "        from pandas_profiling import ProfileReport\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\n",
    "            \"Instale 'ydata-profiling' (recomendado) ou 'pandas-profiling' antes de gerar o relatório.\"\n",
    "        ) from e\n",
    "\n",
    "report = ProfileReport(df)\n",
    "# Mostrar no notebook (iframe) - to_notebook_iframe() já cuida da exibição no Jupyter\n",
    "try:\n",
    "    report.to_notebook_iframe()\n",
    "except Exception:\n",
    "    # Caso a exibição inline falhe, escrevemos o arquivo HTML e informamos o caminho\n",
    "    report.to_file(os.path.join(dir_path, \"preprocessed\", \"profile_report.html\"))\n",
    "    print(f\"Relatório salvo em {os.path.join(dir_path, 'preprocessed', 'profile_report.html')}\")\n",
    "else:\n",
    "    # também salvar em HTML\n",
    "    report.to_file(os.path.join(dir_path, \"preprocessed\", \"profile_report.html\"))\n",
    "\n",
    "# analizando o df de forma truncada as duas colunas iniciais\n",
    "df.describe().iloc[:, :2]  # Tanto `.loc` quanto `.iloc` são atributos essenciais dos DataFrames do Pandas e ambos são usados ​​para selecionar subconjuntos específicos de dados. Seu propósito é acessar e permitir a manipulação de uma parte específica do DataFrame, em vez do DataFrame inteiro.\n",
    "\n",
    "# vamos utilizar o isnull para verificar valores nulos\n",
    "df.isnull().sum()\n",
    "df.isnull().mean()\n",
    "\n",
    "# analise de algumas das linhas com dados ausentes com boleanos\n",
    "mask = df.isnull().any(axis=1)\n",
    "print(mask.sum(), \"linhas com pelo menos um NaN\")\n",
    "display(df[mask].head())           # inspeciona as linhas com NaN\n",
    "print(df.isnull().sum()[lambda s: s>0])  # colunas com NaNs e suas contagens\n",
    "print((df.isnull().mean()*100).sort_values(ascending=False).head())  # % de NaNs por coluna\n",
    "#mask.head() #linhas\n",
    "#df[mask].body.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a0e1957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId      int64\n",
      "Survived         int64\n",
      "Pclass           int64\n",
      "Name            object\n",
      "Sex             object\n",
      "Age            float64\n",
      "SibSp            int64\n",
      "Parch            int64\n",
      "Ticket          object\n",
      "Fare           float64\n",
      "Cabin           object\n",
      "Embarked        object\n",
      "dtype: object\n",
      "a cabeça do nosso objeto names: 0                              Braund, Mr. Owen Harris\n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
      "2                                Heikkinen, Miss Laina\n",
      "Name: Name, dtype: object\n",
      "Nossas colunas são com os dummies aplicados: Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male',\n",
      "       'Embarked_C', 'Embarked_Q', 'Embarked_S'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# categorização para limpeza\n",
    "# defina o target antes de remover a coluna\n",
    "print(df.dtypes)\n",
    "y = df['Survived']   #Estou em dúvida se criaria outra coluna \"dead\" como target alternativo. Se pensarmos em Boleanos, seriam antônimos\n",
    "X = df.drop(columns=\"Survived\")\n",
    "\n",
    "if 'Name' in df.columns:\n",
    "    name= df['Name']\n",
    "    print(f'a cabeça do nosso objeto names: {name.head(3)}')\n",
    "else:\n",
    "    print(\"A coluna do objeto Name está presente somente no dataframe original.\")\n",
    "#removendo colunas que não serão usadas ou podem gerar viés no modelo\n",
    "\n",
    "\n",
    "df=df.drop(columns=[\"Name\",\"Ticket\", \"Survived\", \"Cabin\"])   # Essas colunas não nos dizem muito para o modelo\n",
    "# logo vamos criar uma coluna \"dummy\"\n",
    "df=pd.get_dummies(df)  # ou # df=pd.get_dummies(df,drop_first=True) #para evitar a dummy trap... No caso as colunas male_sex e famale são inversos perfeitos\n",
    "df=df.drop(columns=[\"Sex_female\"])  # removendo uma das colunas dummies para evitar dummy trap\n",
    "print(f'Nossas colunas são com os dummies aplicados: {df.columns}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8895722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação de amostra #amostra_para_validação_do_modelo\n",
    "# scikit-learn para separar 30% para os testes\n",
    "# import library q usaremos abaixo\n",
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Impotação dos dados\n",
    "##como a coluna idade tem valores ausentes. Vamos imputar udade a partir dos valores numéricos apenas no conjunto de treinamento, e então"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da66e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IterativeImputer é experimental em algumas versões do sklearn:\n",
    "# importe o \"enable\" antes de importar o estimador\n",
    "from sklearn.experimental import enable_iterative_imputer  # deve vir antes DO DE BAIXO\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Colunas numéricas a imputar (use nomes como strings)\n",
    "num_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# verificar quais colunas realmente existem nos dataframes (evita KeyError)\n",
    "num_cols = [c for c in num_cols if c in X_train.columns]\n",
    "\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "X_train.loc[:, num_cols] = imputer.fit_transform(X_train[num_cols])\n",
    "X_test.loc[:, num_cols] = imputer.transform(X_test[num_cols])\n",
    "\n",
    "# preencher possíveis NaNs restantes com a mediana do conjunto de treino (aplicado às colunas numéricas)\n",
    "meds = X_train[num_cols].median()\n",
    "X_train.loc[:, num_cols] = X_train[num_cols].fillna(meds)\n",
    "X_test.loc[:, num_cols] = X_test[num_cols].fillna(meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc770223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (623, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61285/1525818701.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-1.63788124  0.80326712  0.80326712 -0.41730706 -0.41730706 -1.63788124\n",
      " -0.41730706  0.80326712  0.80326712 -0.41730706 -1.63788124 -0.41730706\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -1.63788124  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712  0.80326712 -1.63788124 -0.41730706 -0.41730706\n",
      "  0.80326712 -1.63788124  0.80326712  0.80326712 -1.63788124 -0.41730706\n",
      " -1.63788124 -0.41730706 -0.41730706  0.80326712  0.80326712 -0.41730706\n",
      "  0.80326712 -0.41730706 -0.41730706  0.80326712 -1.63788124  0.80326712\n",
      " -1.63788124  0.80326712 -0.41730706  0.80326712  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      " -0.41730706 -1.63788124 -1.63788124 -0.41730706  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712\n",
      " -1.63788124  0.80326712 -1.63788124  0.80326712 -1.63788124  0.80326712\n",
      "  0.80326712 -0.41730706  0.80326712  0.80326712 -1.63788124 -0.41730706\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712  0.80326712 -1.63788124\n",
      " -1.63788124 -0.41730706  0.80326712 -0.41730706  0.80326712  0.80326712\n",
      " -1.63788124  0.80326712 -0.41730706  0.80326712 -1.63788124  0.80326712\n",
      " -1.63788124 -1.63788124  0.80326712  0.80326712 -0.41730706 -1.63788124\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712 -1.63788124 -0.41730706\n",
      " -1.63788124 -1.63788124 -0.41730706  0.80326712 -0.41730706 -0.41730706\n",
      " -1.63788124  0.80326712 -1.63788124  0.80326712 -0.41730706  0.80326712\n",
      " -1.63788124  0.80326712 -0.41730706  0.80326712  0.80326712  0.80326712\n",
      " -0.41730706 -0.41730706 -0.41730706  0.80326712 -1.63788124 -0.41730706\n",
      "  0.80326712 -1.63788124 -0.41730706  0.80326712 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712 -0.41730706  0.80326712 -1.63788124 -0.41730706\n",
      "  0.80326712 -1.63788124  0.80326712 -0.41730706 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124 -1.63788124 -0.41730706 -1.63788124\n",
      " -0.41730706 -0.41730706  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712 -1.63788124 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712\n",
      " -0.41730706  0.80326712 -0.41730706  0.80326712 -1.63788124  0.80326712\n",
      " -0.41730706  0.80326712 -1.63788124 -1.63788124 -1.63788124 -1.63788124\n",
      " -1.63788124  0.80326712 -1.63788124  0.80326712 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712\n",
      " -1.63788124  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712\n",
      " -0.41730706 -1.63788124  0.80326712  0.80326712 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712\n",
      " -1.63788124  0.80326712  0.80326712 -0.41730706 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712 -1.63788124 -0.41730706\n",
      " -0.41730706 -1.63788124  0.80326712  0.80326712 -1.63788124 -1.63788124\n",
      " -1.63788124  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      " -0.41730706  0.80326712 -0.41730706  0.80326712  0.80326712 -1.63788124\n",
      " -0.41730706  0.80326712 -1.63788124  0.80326712  0.80326712 -1.63788124\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      " -0.41730706  0.80326712  0.80326712  0.80326712  0.80326712 -1.63788124\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706 -1.63788124\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712 -0.41730706 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712 -0.41730706  0.80326712 -0.41730706 -1.63788124  0.80326712\n",
      " -1.63788124  0.80326712  0.80326712 -0.41730706  0.80326712 -0.41730706\n",
      "  0.80326712 -1.63788124  0.80326712 -0.41730706  0.80326712 -1.63788124\n",
      "  0.80326712 -0.41730706 -1.63788124  0.80326712 -1.63788124  0.80326712\n",
      " -0.41730706 -0.41730706 -1.63788124 -1.63788124  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      " -1.63788124  0.80326712 -0.41730706 -1.63788124 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712  0.80326712  0.80326712\n",
      " -0.41730706 -0.41730706  0.80326712 -1.63788124  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712 -1.63788124\n",
      "  0.80326712 -0.41730706 -1.63788124 -0.41730706 -0.41730706 -1.63788124\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712\n",
      "  0.80326712 -0.41730706  0.80326712 -1.63788124  0.80326712 -0.41730706\n",
      " -0.41730706 -1.63788124 -1.63788124 -1.63788124 -1.63788124  0.80326712\n",
      "  0.80326712 -1.63788124  0.80326712 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124 -1.63788124 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712 -0.41730706 -0.41730706\n",
      "  0.80326712 -1.63788124  0.80326712  0.80326712  0.80326712  0.80326712\n",
      " -0.41730706  0.80326712 -1.63788124 -0.41730706 -1.63788124  0.80326712\n",
      " -1.63788124  0.80326712 -0.41730706 -1.63788124  0.80326712  0.80326712\n",
      " -1.63788124 -1.63788124 -1.63788124  0.80326712  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712 -1.63788124\n",
      " -1.63788124  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      "  0.80326712 -0.41730706  0.80326712  0.80326712  0.80326712  0.80326712\n",
      "  0.80326712 -0.41730706  0.80326712  0.80326712 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712\n",
      " -0.41730706 -1.63788124  0.80326712 -1.63788124  0.80326712  0.80326712\n",
      " -0.41730706 -0.41730706 -0.41730706  0.80326712  0.80326712 -1.63788124\n",
      "  0.80326712 -1.63788124  0.80326712 -1.63788124  0.80326712 -1.63788124\n",
      " -1.63788124 -1.63788124  0.80326712  0.80326712  0.80326712 -1.63788124\n",
      "  0.80326712 -1.63788124 -1.63788124  0.80326712 -1.63788124 -1.63788124\n",
      " -1.63788124  0.80326712 -1.63788124 -1.63788124 -1.63788124  0.80326712\n",
      " -0.41730706  0.80326712  0.80326712 -1.63788124 -1.63788124  0.80326712\n",
      " -0.41730706 -1.63788124  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -1.63788124  0.80326712  0.80326712\n",
      " -0.41730706  0.80326712 -1.63788124  0.80326712 -1.63788124  0.80326712\n",
      " -0.41730706  0.80326712  0.80326712  0.80326712  0.80326712 -0.41730706\n",
      " -0.41730706 -1.63788124 -1.63788124 -0.41730706  0.80326712  0.80326712\n",
      " -1.63788124  0.80326712 -1.63788124 -0.41730706  0.80326712  0.80326712\n",
      " -0.41730706  0.80326712  0.80326712 -1.63788124  0.80326712 -1.63788124\n",
      " -1.63788124  0.80326712 -0.41730706  0.80326712  0.80326712 -0.41730706\n",
      " -1.63788124 -0.41730706  0.80326712  0.80326712  0.80326712  0.80326712\n",
      " -0.41730706 -0.41730706  0.80326712 -0.41730706  0.80326712 -0.41730706\n",
      "  0.80326712 -0.41730706  0.80326712  0.80326712  0.80326712 -1.63788124\n",
      " -0.41730706  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712\n",
      " -0.41730706  0.80326712  0.80326712 -0.41730706 -1.63788124  0.80326712\n",
      "  0.80326712 -1.63788124  0.80326712 -1.63788124 -1.63788124]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_train.loc[:, num_cols] = sca.fit_transform(X_train[num_cols])\n",
      "/tmp/ipykernel_61285/1525818701.py:10: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.80326712 -0.41730706  0.80326712 -0.41730706  0.80326712 -1.63788124\n",
      "  0.80326712  0.80326712  0.80326712 -1.63788124 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712 -0.41730706 -1.63788124 -1.63788124  0.80326712\n",
      " -0.41730706 -1.63788124  0.80326712 -1.63788124  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124 -0.41730706  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712  0.80326712  0.80326712\n",
      " -1.63788124  0.80326712 -1.63788124  0.80326712 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712\n",
      "  0.80326712 -1.63788124  0.80326712 -1.63788124  0.80326712 -1.63788124\n",
      "  0.80326712 -1.63788124 -0.41730706 -1.63788124 -0.41730706  0.80326712\n",
      "  0.80326712 -0.41730706 -0.41730706 -1.63788124  0.80326712 -0.41730706\n",
      " -0.41730706  0.80326712  0.80326712 -1.63788124 -0.41730706 -1.63788124\n",
      " -1.63788124 -1.63788124  0.80326712  0.80326712  0.80326712 -1.63788124\n",
      " -0.41730706  0.80326712  0.80326712 -0.41730706 -1.63788124  0.80326712\n",
      " -1.63788124 -1.63788124 -1.63788124 -1.63788124 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712  0.80326712  0.80326712\n",
      " -1.63788124  0.80326712 -0.41730706  0.80326712 -1.63788124  0.80326712\n",
      " -1.63788124  0.80326712  0.80326712  0.80326712 -0.41730706 -1.63788124\n",
      "  0.80326712 -1.63788124 -1.63788124  0.80326712 -1.63788124 -1.63788124\n",
      " -1.63788124  0.80326712 -1.63788124 -0.41730706 -0.41730706  0.80326712\n",
      " -0.41730706 -1.63788124 -1.63788124 -0.41730706  0.80326712 -1.63788124\n",
      " -1.63788124 -1.63788124  0.80326712  0.80326712  0.80326712  0.80326712\n",
      " -0.41730706  0.80326712 -1.63788124  0.80326712 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706 -0.41730706 -0.41730706\n",
      "  0.80326712 -0.41730706 -0.41730706 -0.41730706 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -0.41730706  0.80326712  0.80326712 -1.63788124\n",
      " -0.41730706 -1.63788124  0.80326712 -1.63788124 -0.41730706 -1.63788124\n",
      "  0.80326712  0.80326712 -1.63788124 -0.41730706 -1.63788124  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712 -1.63788124\n",
      " -0.41730706  0.80326712  0.80326712  0.80326712 -1.63788124 -1.63788124\n",
      " -0.41730706  0.80326712 -1.63788124 -0.41730706  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712 -0.41730706 -1.63788124 -1.63788124 -0.41730706\n",
      " -1.63788124  0.80326712  0.80326712  0.80326712 -0.41730706  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -1.63788124  0.80326712  0.80326712\n",
      " -1.63788124  0.80326712 -1.63788124  0.80326712  0.80326712  0.80326712\n",
      " -0.41730706 -1.63788124 -0.41730706  0.80326712  0.80326712 -0.41730706\n",
      "  0.80326712 -1.63788124  0.80326712 -0.41730706  0.80326712 -1.63788124\n",
      " -0.41730706  0.80326712 -0.41730706 -1.63788124  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712 -1.63788124 -0.41730706\n",
      "  0.80326712 -0.41730706  0.80326712  0.80326712  0.80326712  0.80326712\n",
      "  0.80326712  0.80326712  0.80326712 -1.63788124  0.80326712 -0.41730706\n",
      "  0.80326712  0.80326712 -1.63788124  0.80326712  0.80326712 -1.63788124\n",
      " -0.41730706 -0.41730706 -1.63788124 -1.63788124 -0.41730706  0.80326712\n",
      " -1.63788124 -1.63788124  0.80326712 -1.63788124]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.loc[:, num_cols] = sca.transform(X_test[num_cols])\n"
     ]
    }
   ],
   "source": [
    "#normalização dos dados\n",
    "## Este processo será importante para nosso modelo ter uma melhor performance.\n",
    "cols = [\"Pclass\", \"Age\", \"Sibsp\", \"Fare\"]\n",
    "num_cols = [c for c in cols if c in X_train.columns]\n",
    "sca = preprocessing.StandardScaler()\n",
    "\n",
    "# Escalar apenas as colunas numéricas selecionadas no lugar, preservando as demais colunas (dummies)\n",
    "if num_cols:\n",
    "    X_train.loc[:, num_cols] = sca.fit_transform(X_train[num_cols])\n",
    "    X_test.loc[:, num_cols] = sca.transform(X_test[num_cols])\n",
    "\n",
    "\n",
    "# Conferir tipos e shapeprint(\"Numeric cols scaled:\", num_cols)\n",
    "print(\"X_train.shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f79ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refatoração\n",
    "## A medida que o ML for evoluindo, vamos precisar realizar ajustes e até mesmo importar o original dataset novamente para refinar o modelo.\n",
    "## Faremos duas funções para o processo\n",
    "\n",
    "def tweak_titanic(df):\n",
    "    \"\"\"Remove colunas que causam leakage e converte categóricas em dummies (drop_first=True).\n",
    "    Retorna um DataFrame somente com colunas numéricas e dummies.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    drop_cols = [c for c in [\"Name\", \"Ticket\", \"Survived\", \"Cabin\"] if c in df.columns]\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols)\n",
    "    df = pd.get_dummies(df, drop_first=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_train_test_X_y(df, y_col, size=0.3, std_cols=None):\n",
    "    \"\"\"Separa X/y, divide treino/teste, imputa colunas numéricas e (opcional) escala colunas em std_cols.\n",
    "    Retorna X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "    if y_col not in df.columns:\n",
    "        raise ValueError(f\"{y_col} não encontrado em df.columns\")\n",
    "\n",
    "    y = df[y_col]\n",
    "    X = df.drop(columns=y_col)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        X, y, test_size=size, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Colunas numéricas candidatas (ajuste conforme seu df)\n",
    "    num_candidates = [\"Pclass\", \"Age\", \"SibSp\", \"Sibsp\", \"Parch\", \"Fare\"]\n",
    "    num_cols = [c for c in num_candidates if c in X_train.columns]\n",
    "\n",
    "    if num_cols:\n",
    "        from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "        from sklearn.impute import IterativeImputer\n",
    "\n",
    "        imputer = IterativeImputer(random_state=0)\n",
    "        X_train.loc[:, num_cols] = imputer.fit_transform(X_train[num_cols])\n",
    "        X_test.loc[:, num_cols] = imputer.transform(X_test[num_cols])\n",
    "\n",
    "    # Normalizar colunas passadas em std_cols (aceita string \"a,b,c\" ou lista)\n",
    "    if std_cols:\n",
    "        if isinstance(std_cols, str):\n",
    "            std_cols = [c.strip() for c in std_cols.split(\",\")]\n",
    "        std_cols = [c for c in std_cols if c in X_train.columns]\n",
    "        if std_cols:\n",
    "            std = preprocessing.StandardScaler()\n",
    "            X_train.loc[:, std_cols] = std.fit_transform(X_train[std_cols])\n",
    "            X_test.loc[:, std_cols] = std.transform(X_test[std_cols])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    # Escalar colunas passadas em std_cols (somente se existirem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1df9afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiago-ubunto/Documentos/Estudo/ti_estudo/meus-projetos/Titanic/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyClassifier        AUC: 0.500 STD: 0.00\n",
      "LogisticRegression     FAILED!!!!!: could not convert string to float: 'Dodge, Master Washington'\n",
      "DecisionTreeClassifier FAILED!!!!!: could not convert string to float: 'Dodge, Master Washington'\n",
      "KNeighborsClassifier   FAILED!!!!!: could not convert string to float: 'Dodge, Master Washington'\n",
      "GaussianNB             FAILED!!!!!: could not convert string to float: 'Dodge, Master Washington'\n",
      "SVC                    FAILED!!!!!: could not convert string to float: 'Dodge, Master Washington'\n",
      "RandomForestClassifier FAILED!!!!!: could not convert string to float: 'Dodge, Master Washington'\n",
      "XGBClassifier          FAILED!!!!!: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Name: object, Sex: object, Ticket: object, Cabin: object, Embarked: object\n"
     ]
    }
   ],
   "source": [
    "#modelo_de_base\n",
    "#uma_base_realmente_simples_que_o_modelo_usara_como_comparador\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "bm = DummyClassifier()\n",
    "bm.fit(X_train, y_train)\n",
    "bm.score(X_test, y_test) #nos_dará_nossa_precisao\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.precision_score(y_test, bm.predict(X_test))\n",
    "\n",
    "\n",
    "\n",
    "#familias\n",
    "#Neste_projeto_vamos_comparar_pontuações_AUC_e_o_desvio_padão_usando_a_validação_cruzadak_fold\n",
    "X=pd.concat([X_train, X_test])\n",
    "y=pd.concat([y_train, y_test])\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = [\n",
    "    DummyClassifier(),\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    KNeighborsClassifier(),\n",
    "    GaussianNB(),\n",
    "    SVC(probability=True),\n",
    "    RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42),\n",
    "\n",
    "    XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "\n",
    "]\n",
    "\n",
    "## Cross-validation stratificada com 10 folds\n",
    "cv = model_selection.StratifiedKFold(n_splits=10, shuffle=True, random_state=42)    \n",
    "\n",
    "for m in models:\n",
    "    try:\n",
    "        s = cross_val_score(m, X, y, scoring=\"roc_auc\", cv=cv, n_jobs=-1, error_score='raise')\n",
    "        print(f'{m.__class__.__name__:22} AUC: {s.mean():.3f} STD: {s.std():.2f}')  #Por que usar o AUC em vez da Acurácia? A acurácia pode ser enganosa se os seus dados estiverem desbalanceados.\n",
    "    except Exception as e:\n",
    "        print(f'{m.__class__.__name__:22} FAILED!!!!!: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f730e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modulo\n",
    "## Usaremos um classificador de random forest\n",
    "rf=ensemble.RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliação de modelo\n",
    "rf.score(X_test, y_test)\n",
    "metrics.precision_score(y_test, rf.predict(X_test))\n",
    "\n",
    "for col, val in sorted(\n",
    "zip(X_train.columns, rf.feature_importances_,), key=lambda x: x[1], reverse=True,)[:5]):\n",
    "    print(f'{col:10}{val:10.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f07614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otimizando o modelo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Titanic (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
